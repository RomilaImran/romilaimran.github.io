<main>
    <section id="project-intro">
        <style>
            /* --- PROJECT 1 STYLES --- */

            /* Reduce the default bottom margin of the main title */
            #project-intro h1 {
                margin-bottom: 10px;
            }

            /* Reduce the default top margin of the paragraph */
            .intro-paragraph {
                margin-top: 10px;
            }

            /* CSS to create the side-by-side layout for the introduction text and the single image */
            .intro-layout-top {
                display: flex; /* Enable flex container */
                align-items: flex-start; /* Align items to the top */
                gap: 20px; /* Space between the text block and the image */
            }

            .intro-text-block {
                /* Allow text block to take up available space */
                flex: 1;
                min-width: 400px;
            }

            /* Style for the single image container */
            .method-image-container img {
                max-width: 550px;
                height: 450px;
                width: auto;
            }

            /* Standard image container for the technique image */
            #methodology .image-row-bottom,
            .image-row-bottom {
                display: flex;
                justify-content: center;
                margin-top: 20px; /* Add space below the preceding text */
            }

            #methodology .image-container img {
                width: 550px;
                height: 450px;
            }

            /* Style for result images in a row */
            #results .image-row {
                display: flex;
                justify-content: center;
                gap: 20px; /* Added gap for better separation of result images */
                flex-wrap: wrap; /* Allow images to wrap on smaller screens */
            }
        </style>

        <h1>Investigating Whole-scalp Accuracy-specific Neural Correlates Underlying Reinforcement Learning</h1>
        <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 15px; text-align: center;">
            Note: All images taken from my work. Code found on
            <a href="https://github.com/RomilaImran/Capstone" target="_blank">GitHub</a>.
            Thesis with parts redacted:
            <a href="https://docs.google.com/document/d/1PRiuKHGVvQWm5iNwaom7YDRGVxY4_K7JM85Ou9aEfds/edit?usp=sharing" target="_blank">
                Google Docs
            </a>.
            PPT:
            <a href="https://docs.google.com/presentation/d/1Ep65J73TX6BLkdnRNjzRQ1ARUH9V-s5gOQ48-9Zme64/edit?usp=sharing" target="_blank">
                Google Slides
            </a>.
        </p>


        <div class="intro-layout-top">
            <div class="intro-text-block">
                <p class="intro-paragraph">
                    While the role of positive feedback in reinforcement learning (e.g., maintain strategy) is well-characterized, the exact function of negative (incorrect) feedback remains ambiguous, variously suggested to serve as performance monitoring, a signal for strategy change, or simply an inverted form of positive feedback. Resolving this ambiguity was a key goal. Second, existing research is limited by its heavy reliance on univariate Event-Related Potentials (ERPs), which overemphasize individual electrode activity and neglect complex, network-level brain processes that arise from several overlapping ERPs. Since learning is a dynamic and distributed task, this approach risks overlooking crucial signals. Therefore, the primary objective of this project was to overcome these methodological limitations by employing a <b>multivariate, whole-scalp approach</b> to identify distinct neural correlates for positive and negative feedback, and subsequently to correlate these specific neural signatures with the <b>change in the learning rate</b> to determine their functional relevance in adaptive learning.
                </p>

                <p>
                    The trial data utilized for this study was collected from <b>22 right-handed participants</b> (ages 18-29), who performed a two-alternative forced-choice (2AFC) task over three consecutive days at the University of Leeds. The task required participants to differentiate between visual, auditory, and audiovisual stimuli (V, A, and AV) of either a car or a face. Each participant completed 216 trials daily, totaling 648 trials across the experiment. Stimulus clarity was systematically varied using low (32.5%) and high (37.6%) phase coherence, with immediate, valence-specific feedback (smiling or frowning cartoon scientist) provided upon response. Neural activity was recorded from <b>64 electrodes</b> placed according to the International 10-20 system at a 1000 Hz sampling rate, yielding a substantial total of <b>912,384 individual electrode recordings</b> across all participants and trials. To isolate the neural correlates of decision-making and feedback processing, the EEG data was <b>response-locked</b>, setting 0 ms to the simultaneous occurrence of the participant's keypress and the onset of feedback. The analysis subsequently focused on the <b>450 ms pre-response to 1000 ms post-response</b> window, utilizing only Day 1 and Day 3 data to maximize the contrast in learning progression.
                </p>
            </div>

            <div class="method-image-container">
                <img src="images/method1.PNG" alt="Figure 1: Illustration of the experimental setup, showing a participant undergoing EEG recording during the reinforcement learning task." class="project-image" width="100" height="100">
            </div>
        </div>
    </section>

    <section id="methodology">

        <h2>Technique</h2>

        <div style="display: flex; align-items: flex-start; gap: 20px;">
            <div style="flex: 1; min-width: 400px;">
                <p>
                    Multivariate neural signatures were deduced using <b>Linear Discriminant Analysis (LDA)</b> classifiers, trained separately on correct and incorrect whole-scalp EEG activity to identify features best distinguishing between learning states on Day 1 and Day 3. The classifier worked by optimizing a hyperplane in sequential 70 ms time windows to maximize separation between the two days. The performance of this distinction was measured by the <b>Az score</b> (Area Under the ROC Curve); time points with high Az scores indicate a neural correlate where significant learning-related change occurred. The classifier's continuous output, the <b>discriminant score</b>, was extracted as the neural signature. These signatures were spatially interpreted using scalp maps derived from the classifier's weight vector (importance of each electrode) via <b>Haufe's transformation</b> over the <b>-450 ms pre-response to 1000 ms post-response</b> period.
                </p>

                <p>
                    To quantify the behavioral aspect of learning, a <b>Reinforcement Learning Drift-Diffusion Model (RLDDM)</b> was employed, which extends the standard DDM to include the impact of feedback (Q-learning) on future decisions. This model estimated the <b>learning rate</b> for each participant, a core parameter quantifying a participant's responsiveness to reward. The behavioral measure of adaptive learning was calculated as the <b>change in learning rate</b> between Day 3 and Day 1. The functional relevance of the neural correlates was then determined by using <b>Pearson's correlation</b> to test for a linear relationship between the mean <b>discriminant score</b> at the highest Az peaks and the magnitude of <b>adaptive learning</b> (change in learning rate).
                </p>

                <h4>1. Multivariate EEG Analysis:</h4>
                <ul>
                    <li>Technique: A supervised single-trial analysis using <b>Linear Discriminant Analysis (LDA)</b> was employed to deduce multivariate neural signatures.</li>
                    <li>Process: Separate classifiers were trained to identify whole-scalp spatiotemporal EEG activity that best discriminated between learning on Day 1 and Day 3 for both correct and incorrect trials.</li>
                    <li>Output: The continuous classifier output, or <b>discriminant score</b>, was extracted as the neural signature for correlation.</li>
                </ul>

                <h4>2. Behavioral Modeling:</h4>
                <ul>
                    <li>A <b>Reinforcement Learning Drift-Diffusion Model (RLDDM)</b> was used, which models both decision-making (evidence accumulation) and learning-related parameters.</li>
                    <li>Focus Parameter: The RLDDM's <b>learning rate</b> was derived to quantify how responsive a participant was to feedback, as this parameter is highly relevant to neural processes that update belief structures based on feedback.</li>
                    <li>Measure of Adaptation: The <b>change in learning rate</b> between Day 1 and Day 3 was calculated and used as the behavioral measure of adaptive learning.</li>

                </ul>

                <h4>3. Functional Correlation:</h4>
                <ul>
                    <li>The neural signature (discriminant score) was correlated with the <b>change in the learning rate</b> using <b>Pearson's correlation</b> to establish functional relevance.</li>
                </ul>
            </div>

            <div class="image-container">
                <img src="images/technique1.PNG" alt="Figure 2: Whole-scalp topographical map highlighting regions involved in data acquisition." class="project-image" width="500" height="400">
            </div>
        </div>

    </section>

    <section id="results">
        <h2>Findings</h2>
        <p>
            Based on the <b>Multivariate Linear Discriminant Analysis (LDA)</b> of whole-scalp activity, four primary neural correlates (<b>Az scores</b>) were identified: <b>C1</b> (<b>-319</b> ms, pre-response correct), <b>I1</b> (<b>-409</b> ms, pre-response incorrect), <b>C2</b> (<b>391</b> ms, post-response correct), and <b>I2</b> (<b>581</b> ms, post-response incorrect), alongside signals at the feedback onset (<b>C0/I0</b> at 0 ms).

            Spatially, correct trials exhibited consistent and compact signals, transitioning from <b>occipital activity</b> at feedback onset (0 ms) to <b>frontoparietal regions</b> post-response (~ 150 to 950 ms). Conversely, incorrect trials yielded more diffused signals with notable <b>right-lateralized frontal activity</b> post-response, indicative of heightened frontal control and affective processing.

            When assessing functional relevance by correlating these signatures with the <b>change in the learning rate</b>, <b>positive feedback correlates (C1 and C0) proved highly relevant</b>, showing a significant correlation with the <b>change in learning rate (correct trials)</b> across both Day 1 and Day 3. This enduring correlation suggests that these signals, particularly the C1 and C0 activity which involves occipital regions and may reflect anticipatory prediction or confidence, actively <b>track adaptive learning progression</b>.

            The post-response correlate, C2, was only relevant on Day 1, suggesting it captures the initial, greater need for cognitive updating (a P3b-like response) that diminishes as learning becomes automated. Crucially, none of the <b>incorrect feedback correlates (I1, I0, I2) showed a significant linear correlation</b> with the <b>change in learning rate (incorrect trials)</b>, implying they do not linearly track the magnitude of learning adaptation, despite their distinct spatial patterns (e.g., right frontal/parietal activity) suggesting systemic involvement in attention and conflict monitoring.
        </p>

        <div class="image-row">
            <div class="image-container">
                <img src="images/result1.PNG" alt="Figure 3: Whole-scalp topographical map showing regions of heightened neural activity during accurate feedback processing." class="project-image" width="550" height="550">
            </div>
            <div class="image-container">
                <img src="images/plot1.PNG" alt="Figure 4: Bar graph illustrating differences in specific ERP components for accurate vs. inaccurate outcomes." class="project-image" width="520" height="500">
            </div>
        </div>
    </section>

    <section id="project-two">
    <style>
        /* --- PROJECT 2 STYLES (Gallery Focus) --- */

        /* Styles for the main title */
        #project-two h1 {
            margin-top: 50px; /* Space above the new project title */
            margin-bottom: 15px;
            text-align: center; /* Center the title for a clean look */
        }

        /* Styles for the image gallery rows */
        .image-gallery-row {
            display: flex;
            justify-content: center; /* Center the entire row of images */
            align-items: flex-start; /* Align images at the top */
            gap: 20px; /* Space between images */
            margin-bottom: 30px; /* Space between rows */
            flex-wrap: wrap; /* Important for responsiveness */
        }

        .image-gallery-row .gallery-image-container {
            /* MAXIMUM PRACTICAL SIZE HERE: increased from 400px to 450px */
            flex: 0 1 450px; /* Allow items to shrink/grow slightly but maintain a base width */
            text-align: center;
        }

        .image-gallery-row .gallery-image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd; /* Subtle border for definition */
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }

        /* Remove or repurpose unused styles from the original template */
        .project-two-intro-layout { display: none; } /* Hide the intro layout */
        .project-two-results-row { display: none; } /* Hide the old results row */
    </style>

    <h1>Markerless Pose Estimation of Bumblebee Tracks During Sensory-Stimulating Tasks</h1>
    <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 30px; text-align: center;">
        Note: All images taken from my work. Code found on
        <a href="[LINK TO PROJECT 2 GITHUB]" target="_blank">GitHub</a>.
        Paper/Summary:
        <a href="[LINK TO PROJECT 2 DOCS/PAPER]" target="_blank">
            Google Docs/PDF
        </a>.
    </p>

    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <img src="images_bee/slide1.png" alt="Bumblebee tracking setup and initial frame." class="project-image">
        </div>
        <div class="gallery-image-container">
            <img src="images_bee/slide2.png" alt="DeepLabCut labeling interface for bee key points." class="project-image">
        </div>
        <div class="gallery-image-container">
            <img src="images_bee/slide3.png" alt="Visualization of the estimated pose key points on the bee." class="project-image">
        </div>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <img src="images_bee/slide4.png" alt="Graph showing pose estimation accuracy or loss over training." class="project-image">
        </div>
        <div class="gallery-image-container">
            <img src="images_bee/slide5.png" alt="Diagram illustrating the sensory stimulation task apparatus." class="project-image">
        </div>
        <div class="gallery-image-container">
            <img src="images_bee/slide6.png" alt="Example of a tracked bumblebee path during the task." class="project-image">
        </div>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container" style="flex: 0 1 450px;">
            <img src="images_bee/slide7.png" alt="Statistical analysis of bee movement metrics." class="project-image">
        </div>
        <div class="gallery-image-container" style="flex: 0 1 450px;">
            <img src="images_bee/slide8.png" alt="Final visualization of behavior results." class="project-image">
        </div>
        <div class="gallery-image-container" style="flex: 0 1 450px; visibility: hidden;"></div>
    </div>

</section>
</main>
