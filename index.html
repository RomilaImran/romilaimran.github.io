<main>
    <section id="header-bio">
        <style>
            /* --- HEADER BIO STYLES (NEW SECTION) --- */
            #header-bio {
                max-width: 1200px; /* Match project sections max-width */
                margin: 40px auto 30px auto; /* Top, auto for center, bottom margin */
                padding: 20px 40px;
                text-align: center;
                border-bottom: 3px solid #f0f0f0; /* Subtle separator line */
                /* Optional: box-shadow for a lifted effect */
                /* box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05); */
            }

            #header-bio h1 {
                font-size: 2.5em;
                margin-bottom: 5px;
                color: #333; /* Darker text for main name */
            }

            .bio-details-row {
                display: flex;
                flex-wrap: wrap; /* Ensure it wraps on small screens */
                justify-content: center;
                gap: 20px 40px; /* Vertical and horizontal gap */
                margin-top: 15px;
                font-size: 1.1em;
                color: #555;
            }

            .bio-details-row span {
                /* Style for individual detail blocks */
                padding: 5px 0;
            }

            .passion-list {
                margin-top: 20px;
                padding: 10px;
                /* Optional: Light background for the passion section */
                /* background-color: #fafafa; */
                border-radius: 5px;
            }

            .passion-list h3 {
                margin-top: 0;
                margin-bottom: 10px;
                font-size: 1.3em;
                color: #444;
            }

            .passion-list p {
                font-style: italic;
                font-size: 1em;
                line-height: 1.5;
                color: #666;
            }

            body {
                background-color: #fffbe6; /* Very light yellow/creamy white */
            }
            /* ------------------------------------- */
        </style>

        <h1>Romila Imran</h1>

        <div class="bio-details-row" style="flex-direction: column; gap: 5px; font-size: 1.05em; margin-top: 10px;">
            <p style="margin: 0;"><strong>BSc in Neuroscience (Hons)</strong></p>
<p style="margin: 0;"><strong>GPA/ Classification:</strong> 4.0/4.0, First Class Honors</p>
<p style="margin: 0;"><strong>President of Neural Networks (Neuroscience)</strong> @ Leeds (2022-2023)</p>
<!-- UPDATED LINE WITH ARTICLE LINKS -->
<p style="margin: 0;">
    <strong>Writer</strong> @ Neuro Elevation (2022-2023); 
    <a href="https://neuroelevation.wixsite.com/website-1/post/neuralink-show-and-tell-2022" target="_blank">[1]</a>, 
    <a href="https://neuroelevation.wixsite.com/website-1/post/does-the-brain-perform-quantum-computations" target="_blank">[2]</a>, 
    <a href="https://neuroelevation.wixsite.com/website-1/post/lewy-body-dementia-lbd" target="_blank">[3]</a>
</p>
<p style="margin: 0;"><strong>Volunteer</strong> @ Bio-Inspired Sensing, Computing, and Control International Teams (SOAR/BISCCIT) conference @ Imperial</p>
<p style="margin: 0;">
    <strong>Github:</strong> <strong><a href="https://github.com/RomilaImran" target="_blank">here</a></strong>
</p>
        </div>

        <div class="passion-list">
            <h3>Passions:</h3>
            <p>
                Neural interfaces and Neurotechnology, Computational Neuroscience, Electrophysiology, Neurobiology, Biophysics
            </p>
        </div>

    </section>

    <section id="project-intro">
    <style>
        /* --- General Layout and Margin Edits (NEW CODE) --- */
        /* This applies a max-width and centers the content, creating the side margins */
        #project-intro,
        #methodology,
        #results {
            max-width: 1200px; /* Center the container horizontally */
            margin-left: auto;
            margin-right: auto;
            padding: 0 20px; /* Adds small padding on the sides for smaller screens */
        }
        /* ------------------------------------------------ */

        /* --- PROJECT 1 STYLES --- */

        /* Reduce the default bottom margin of the main title */
        #project-intro h1 {
            margin-bottom: 10px;
            text-align: center;
        }

        /* Reduce the default top margin of the paragraph */
        .intro-paragraph {
            margin-top: 10px;
        }

        /* CSS to create the side-by-side layout for the introduction text and the single image */
        .intro-layout-top {
            display: flex; /* Enable flex container */
            align-items: flex-start; /* Align items to the top */
            gap: 20px; /* Space between the text block and the image */
        }

        .intro-text-block {
            /* Allow text block to take up available space */
            flex: 1;
            min-width: 400px;
        }

        /* Style for the single image container */
        .method-image-container img {
            max-width: 550px;
            height: 450px;
            width: auto;
            cursor: pointer; /* ADDED: Makes the cursor a pointer to indicate clickability */
        }

        /* Standard image container for the technique image */
        #methodology .image-row-bottom,
        .image-row-bottom {
            display: flex;
            justify-content: center;
            margin-top: 20px; /* Add space below the preceding text */
        }

        /* ADDED: Styling for the method image to be clickable */
        #methodology .image-container img {
            width: 550px;
            height: 450px;
            cursor: pointer; /* ADDED: Makes the cursor a pointer to indicate clickability */
        }

        /* Style for result images in a row */
        #results .image-row {
            display: flex;
            justify-content: center;
            gap: 20px; /* Added gap for better separation of result images */
            flex-wrap: wrap; /* Allow images to wrap on smaller screens */
        }
        
        /* ADDED: Styling for the result images to be clickable */
        #results .image-container img {
            cursor: pointer; /* ADDED: Makes the cursor a pointer to indicate clickability */
        }
    </style>

    <h1>Investigating Whole-scalp Accuracy-specific Neural Correlates Underlying Reinforcement Learning</h1>
    <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 15px; text-align: center;">
        Note: All images taken from my work. Code found on
        <a href="https://github.com/RomilaImran/Capstone" target="_blank">GitHub</a>.
        Thesis with parts redacted:
        <a href="https://docs.google.com/document/d/1PRiuKHGVvQWm5iNwaom7YDRGVxY4_K7JM85Ou9aEfds/edit?usp=sharing" target="_blank">
            Google Docs
        </a>.
        PPT:
        <a href="https://docs.google.com/presentation/d/1Ep65J73TX6BLkdnRNjzRQ1ARUH9V-s5gOQ48-9Zme64/edit?usp=sharing" target="_blank">
            Google Slides
        </a>.
        <p>Supervisor: Dr Ioannis Delis @ Motor Control and Neurorehabilitation research team University of Leeds.</p>
    </p>


    <div class="intro-layout-top">
        <div class="intro-text-block">
            <p class="intro-paragraph">
                While the role of positive feedback in reinforcement learning (e.g., maintain strategy) is well-characterized, the exact function of negative (incorrect) feedback remains ambiguous, variously suggested to serve as performance monitoring, a signal for strategy change, or simply an inverted form of positive feedback. Resolving this ambiguity was a key goal. Second, existing research is limited by its heavy reliance on univariate Event-Related Potentials (ERPs), which overemphasize individual electrode activity and neglect complex, network-level brain processes that arise from several overlapping ERPs. Since learning is a dynamic and distributed task, this approach risks overlooking crucial signals. Therefore, the primary objective of this project was to overcome these methodological limitations by employing a <b>multivariate, whole-scalp approach</b> to identify distinct neural correlates for positive and negative feedback, and subsequently to correlate these specific neural signatures with the <b>change in the learning rate</b> to determine their functional relevance in adaptive learning.
            </p>

            <p>
                The trial data utilized for this study was collected from <b>22 right-handed participants</b> (ages 18-29), who performed a two-alternative forced-choice (2AFC) task over three consecutive days at the University of Leeds. The task required participants to differentiate between visual, auditory, and audiovisual stimuli (V, A, and AV) of either a car or a face. Each participant completed 216 trials daily, totaling 648 trials across the experiment. Stimulus clarity was systematically varied using low (32.5%) and high (37.6%) phase coherence, with immediate, valence-specific feedback (smiling or frowning cartoon scientist) provided upon response. Neural activity was recorded from <b>64 electrodes</b> placed according to the International 10-20 system at a 1000 Hz sampling rate, yielding a substantial total of <b>912,384 individual electrode recordings</b> across all participants and trials. To isolate the neural correlates of decision-making and feedback processing, the EEG data was <b>response-locked</b>, setting 0 ms to the simultaneous occurrence of the participant's keypress and the onset of feedback. The analysis subsequently focused on the <b>450 ms pre-response to 1000 ms post-response</b> window, utilizing only Day 1 and Day 3 data to maximize the contrast in learning progression.
            </p>
        </div>

        <div class="method-image-container">
            <a href="images/method1.PNG" target="_blank">
                <img src="images/method1.PNG" alt="Figure 1: Illustration of the experimental setup, showing a participant undergoing EEG recording during the reinforcement learning task." class="project-image" width="100" height="100">
            </a>
        </div>
    </div>
</section>

<section id="methodology">

    <h2>Technique</h2>

    <div style="display: flex; align-items: flex-start; gap: 20px;">
        <div style="flex: 1; min-width: 400px;">
            <p>
                Multivariate neural signatures were deduced using <b>Linear Discriminant Analysis (LDA)</b> classifiers, trained separately on correct and incorrect whole-scalp EEG activity to identify features best distinguishing between learning states on Day 1 and Day 3. The classifier worked by optimizing a hyperplane in sequential 70 ms time windows to maximize separation between the two days. The performance of this distinction was measured by the <b>Az score</b> (Area Under the ROC Curve); time points with high Az scores indicate a neural correlate where significant learning-related change occurred. The classifier's continuous output, the <b>discriminant score</b>, was extracted as the neural signature. These signatures were spatially interpreted using scalp maps derived from the classifier's weight vector (importance of each electrode) via <b>Haufe's transformation</b> over the <b>-450 ms pre-response to 1000 ms post-response</b> period.
            </p>

            <p>
                To quantify the behavioral aspect of learning, a <b>Reinforcement Learning Drift-Diffusion Model (RLDDM)</b> was employed, which extends the standard DDM to include the impact of feedback (Q-learning) on future decisions. This model estimated the <b>learning rate</b> for each participant, a core parameter quantifying a participant's responsiveness to reward. The behavioral measure of adaptive learning was calculated as the <b>change in learning rate</b> between Day 3 and Day 1. The functional relevance of the neural correlates was then determined by using <b>Pearson's correlation</b> to test for a linear relationship between the mean <b>discriminant score</b> at the highest Az peaks and the magnitude of <b>adaptive learning</b> (change in learning rate).
            </p>

            <h4>1. Multivariate EEG Analysis:</h4>
            <ul>
                <li>Technique: A supervised single-trial analysis using <b>Linear Discriminant Analysis (LDA)</b> was employed to deduce multivariate neural signatures.</li>
                <li>Process: Separate classifiers were trained to identify whole-scalp spatiotemporal EEG activity that best discriminated between learning on Day 1 and Day 3 for both correct and incorrect trials.</li>
                <li>Output: The continuous classifier output, or <b>discriminant score</b>, was extracted as the neural signature for correlation.</li>
            </ul>

            <h4>2. Behavioral Modeling:</h4>
            <ul>
                <li>A <b>Reinforcement Learning Drift-Diffusion Model (RLDDM)</b> was used, which models both decision-making (evidence accumulation) and learning-related parameters.</li>
                <li>Focus Parameter: The RLDDM's <b>learning rate</b> was derived to quantify how responsive a participant was to feedback, as this parameter is highly relevant to neural processes that update belief structures based on feedback.</li>
                <li>Measure of Adaptation: The <b>change in learning rate</b> between Day 1 and Day 3 was calculated and used as the behavioral measure of adaptive learning.</li>

            </ul>

            <h4>3. Functional Correlation:</h4>
            <ul>
                <li>The neural signature (discriminant score) was correlated with the <b>change in the learning rate</b> using <b>Pearson's correlation</b> to establish functional relevance.</li>
            </ul>
        </div>

        <div class="image-container">
            <a href="images/technique1.PNG" target="_blank">
                <img src="images/technique1.PNG" alt="Figure 2: Whole-scalp topographical map highlighting regions involved in data acquisition." class="project-image" width="500" height="400">
            </a>
        </div>
    </div>

</section>

<section id="results">
    <h2>Findings</h2>
    <p>
        Based on the <b>Multivariate Linear Discriminant Analysis (LDA)</b> of whole-scalp activity, four primary neural correlates (<b>Az scores</b>) were identified: <b>C1</b> (<b>-319</b> ms, pre-response correct), <b>I1</b> (<b>-409</b> ms, pre-response incorrect), <b>C2</b> (<b>391</b> ms, post-response correct), and <b>I2</b> (<b>581</b> ms, post-response incorrect), alongside signals at the feedback onset (<b>C0/I0</b> at 0 ms).

        Spatially, correct trials exhibited consistent and compact signals, transitioning from <b>occipital activity</b> at feedback onset (0 ms) to <b>frontoparietal regions</b> post-response (~ 150 to 950 ms). Conversely, incorrect trials yielded more diffused signals with notable <b>right-lateralized frontal activity</b> post-response, indicative of heightened frontal control and affective processing.

        When assessing functional relevance by correlating these signatures with the <b>change in the learning rate</b>, <b>positive feedback correlates (C1 and C0) proved highly relevant</b>, showing a significant correlation with the <b>change in learning rate (correct trials)</b> across both Day 1 and Day 3. This enduring correlation suggests that these signals, particularly the C1 and C0 activity which involves occipital regions and may reflect anticipatory prediction or confidence, actively <b>track adaptive learning progression</b>.

        The post-response correlate, C2, was only relevant on Day 1, suggesting it captures the initial, greater need for cognitive updating (a P3b-like response) that diminishes as learning becomes automated. Crucially, none of the <b>incorrect feedback correlates (I1, I0, I2) showed a significant linear correlation</b> with the <b>change in learning rate (incorrect trials)</b>, implying they do not linearly track the magnitude of learning adaptation, despite their distinct spatial patterns (e.g., right frontal/parietal activity) suggesting systemic involvement in attention and conflict monitoring.
    </p>

    <div class="image-row">
        <div class="image-container">
            <a href="images/result1.PNG" target="_blank">
                <img src="images/result1.PNG" alt="Figure 3: Whole-scalp topographical map showing regions of heightened neural activity during accurate feedback processing." class="project-image" width="550" height="550">
            </a>
        </div>
        <div class="image-container">
            <a href="images/plot1.PNG" target="_blank">
                <img src="images/plot1.PNG" alt="Figure 4: Bar graph illustrating differences in specific ERP components for accurate vs. inaccurate outcomes." class="project-image" width="520" height="500">
            </a>
        </div>
    </div>
</section>

<section id="project-two">
    <style>
        /* --- PROJECT 2 STYLES (Gallery Focus) --- */

        /* Styles for the main title */
        #project-two h1 {
            margin-top: 50px; /* Space above the new project title */
            margin-bottom: 15px;
            text-align: center; /* Center the title for a clean look */
        }
        
        /* ADDED: Style for the header content block in project two to match the 150px padding */
        .project-two-header-content {
            width: 100%;
            box-sizing: border-box;
            /* Apply the 150px margin effect to the header area */
            padding: 0 150px; 
            margin-bottom: 30px;
        }


        /* NEW: Style for the large-margined text section */
        .project-two-text-section {
            width: 100%;
            box-sizing: border-box;
            /* CRITICAL CHANGE: Use padding to create the large internal space */
            padding: 0 150px 30px 150px;
            text-align: left;
        }

        /* Styles for the image gallery rows */
        .image-gallery-row {
            display: flex;
            justify-content: center; /* Center the entire row of images */
            align-items: flex-start; /* Align images at the top */
            gap: 20px; /* Space between images */
            margin-bottom: 30px; /* Space between rows */
            flex-wrap: wrap; /* Important for responsiveness */
        }

        .image-gallery-row .gallery-image-container {
            /* MAXIMUM PRACTICAL SIZE HERE: 450px */
            flex: 0 1 450px; /* Allow items to shrink/grow slightly but maintain a base width */
            text-align: center;
        }

        .image-gallery-row .gallery-image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd; /* Subtle border for definition */
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
            cursor: pointer; /* Change cursor to indicate clickability */
        }

        /* Remove or repurpose unused styles from the original template */
        .project-two-intro-layout { display: none; } /* Hide the intro layout */
        .project-two-results-row { display: none; } /* Hide the old results row */
    </style>

    <!-- NEW WRAPPER ADDED TO APPLY 150PX MARGIN TO HEADER CONTENT -->
    <div class="project-two-header-content">
        <h1>Kinematic Correlates of Visual Decision-making in Bumblebees for Inferring Cognitive Strategies of Color and Shape Perception</h1>
        <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 0px; text-align: center;">
            Note: All images taken from my work. Code found on
            <a href="[LINK TO PROJECT 2 GITHUB]" target="_blank">GitHub</a>. Supervisor: Dr. Mai Morimoto, Dr. Huai-Ti Lin Lab @ Imperial College London, Bioengineering.
        </p>
    </div>

    <div class="project-two-text-section">
        <p>How does the bumblebee brain process colors and shapes and can this information be inferred from their behaviour if introduced to a task where they are incentivised to choose specific shapes and colors?</p>
        <p><b>Markless pose estimation</b> of bumblebee trajectories in 2D space was performed by training a model to identify the head, thorax, yellow band, and tailpoint of several bumblebees as they performed the task shown in figure 1. This was done with 2 cameras positioned to angle up and down. 2D trajectories from these angles were used to configure the <b>3D reconstruction of the bumblebee’s trajectory</b>. Pose estimation model training was performed using <b></b>DeepLabCut (Mathis et al., 2018)</b> on a <b>ResNet-50 with 496000 training iterations</b>, followed by 3D reconstruction. Pose estimation and 3D reconstruction were also attempted on Anipose for result quality comparison. Model can be found <a href="https://drive.google.com/drive/folders/1xD71fPoi_r_kLBXHtQfpdMviTBJgf0qK?usp=drive_link" target="_blank">here</a> (config.yaml file is relevant). A behavioural code was generated to classify acceptances and rejections by segmenting the video into 16 sections, outlining the 8 feeders, and determining the direction and speed of the bumblebees (figure 6, 7 & 8).</p>
        <p>Figure 9, 10 and 11 are just an example of the trajectory of a bumblebee from a test video after being trained to land on 45° angled stimuli and a density map of its most common position (scanning position) before landing on the correct (45°) and incorrect (135°) feeders. This project is ongoing at Imperial as of 2025 and is awaiting a publication.</p>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images_b/slide1.png" target="_blank">
                <img src="images_b/slide1.png" alt="Bumblebee tracking setup and initial frame." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide2.png" target="_blank">
                <img src="images_b/slide2.png" alt="DeepLabCut labeling interface for bee key points." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide3.png" target="_blank">
                <img src="images_b/slide3.png" alt="Visualization of the estimated pose key points on the bee." class="project-image">
            </a>
        </div>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images_b/slide4.png" target="_blank">
                <img src="images_b/slide4.png" alt="Graph showing pose estimation accuracy or loss over training." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide5.png" target="_blank">
                <img src="images_b/slide5.png" alt="Diagram illustrating the sensory stimulation task apparatus." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide6.png" target="_blank">
                <img src="images_b/slide6.png" alt="Example of a tracked bumblebee path during the task." class="project-image">
            </a>
        </div>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container" style="flex: 0 1 450px;">
            <a href="images_b/slide7.png" target="_blank">
                <img src="images_b/slide7.png" alt="Statistical analysis of bee movement metrics." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container" style="flex: 0 1 450px;">
            <a href="images_b/slide8.png" target="_blank">
                <img src="images_b/slide8.png" alt="Final visualization of behavior results." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide9.png" target="_blank">
                <img src="images_b/slide9.png" alt="Additional data visualization or experimental result 1." class="project-image">
            </a>
        </div>
    </div>

    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images_b/slide10.png" target="_blank">
                <img src="images_b/slide10.png" alt="Additional data visualization or experimental result 2." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images_b/slide12.png" target="_blank">
                <img src="images_b/slide12.png" alt="Additional data visualization or experimental result 3." class="project-image">
            </a>
        </div>
    </div>

</section>

<section id="project-three">
    <style>
        /* --- General Layout and Margin Edits (Inherited from the top section) --- */
        /* This applies a max-width and centers the content, creating the side margins */
        #project-three {
            max-width: 1200px; /* Adjust this value to make the content wider or narrower */
            margin-left: auto; /* Center the container horizontally */
            margin-right: auto; /* Center the container horizontally */
            padding: 0 20px; /* Adds small padding on the sides for smaller screens */
            margin-top: 50px; /* Add space above the new project title */
            margin-bottom: 50px; /* Add space below the section */
        }
        /* ------------------------------------------------ */

        /* --- PROJECT 3 STYLES --- */

        /* Styles for the main title */
        #project-three h1 {
            margin-bottom: 15px;
            text-align: center; /* Center the title for a clean look */
        }

        /* Style for the text block to match the large-margined effect of Project 2 */
        .project-three-text-section {
            width: 100%;
            box-sizing: border-box;
            /* The parent #project-three handles the 20px margin, so this element should not have the extra 150px padding. */
            padding: 0 0 30px 0; /* Only keep bottom padding */
            text-align: left;
        }

        /* Style for the comparison result images */
        .comparison-image-row {
            display: flex;
            justify-content: center; /* Center the entire row of images */
            gap: 40px; /* Increased gap for clear separation */
            margin-top: 20px;
            flex-wrap: wrap; /* Allow images to wrap on smaller screens */
        }

        .comparison-image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
            cursor: pointer;
        }

        /* UPDATED STYLES FOR THE PONDERING PEACOCK IMAGE TO MATCH TEXT WIDTH */
        .llm-peacock-wrapper {
            /* This wrapper is now set up to match the effective content width */
            text-align: center;
            margin-top: 20px; /* Space above the image */
            /* Crucially, we remove unnecessary fixed width and let the image fill the space */
        }

        .llm-peacock-wrapper a {
            display: inline-block; /* Changed to inline-block to respect the centered text-align in the wrapper */
        }

        .llm-peacock-wrapper img {
            /* *** CHANGE HERE: Added max-width to reduce image size *** */
            width: 100%; /* Allows it to be responsive on smaller screens */
            max-width: 600px; /* Reduces the image size to a maximum of 600px */
            height: auto; /* Maintain aspect ratio */
            cursor: pointer;
            border: 1px solid #ddd; /* Added border for visual separation */
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
    </style>

    <h1>Leveraging Large Language Models in Selection Systems - 2023</h1>
    <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 30px; text-align: center;">
        Note: Project completed in 2023 @ GenerationSuccess & University of Leeds. Supervisor: Dr. Samit Chakrabarty. Parts of this report can be viewed with parts redacted <a href="https://docs.google.com/document/d/17PFg5QgOGMnfPhb4aPFqrgcbxAQ5ltdy31hbjLmMAYQ/edit?tab=t.0">here</a>
    </p>

    <div class="project-three-text-section">
        <p>GenerationSuccess is a small social enterprise start-up in London. It is an equity recruitment firm that works with other firms, such as PWC and Amazon. Although the company dealt with millions of data units annually or per recruitment cycle, its processes were still semi-automated, which wasted a lot of time on unnecessary steps. I worked alongside others to contribute toward automating their pipeline, recruitment analytics, and using data from current successful candidates to fuel internal processes. This process involved creating, editing and using simple embedding models to shortlist the success likelihood of potential candidates. I also formally conducted written and oral technical interviews in Python for some undergraduate internships after proving my qualification to the team.</p>
        <p>Apart from my other work, I also undertook this separate non-technical project. Based on my first-hand, behind-the-scenes experience and this occurring just months after first finding out about the revolutionary GPT-3.5, I decided to highlight the importance and considerations of integrating such architectures into selection systems (an application that exceeds HR in practice). I achieved this by writing a thesis that was marked with high consideration by my supervisor, Dr. Samit Chakrabarty, and internal examiners from the University.</p>
    </div>

    <div class="llm-peacock-wrapper">
        <a href="images/ponderingpeacock_48x24.png" target="_blank">
            <img src="images/ponderingpeacock_48x24.png" alt="Pondering Peacock icon">
        </a>
    </div>

</section>

   <section id="project-four">
    <style>
        /* --- PROJECT 4 STYLES (Gallery Focus) --- */

        /* Styles for the main title */
        #project-four h1 {
            margin-top: 50px;
            margin-bottom: 15px;
            text-align: center;
        }
        
        /* Style for the header content block in project four (Title/Instructor) */
        .project-four-header-content {
            width: 100%;
            box-sizing: border-box;
            /* INCREASED PADDING for 'bigger margins' (200px) */
            padding: 0 200px; 
            margin-bottom: 30px;
            text-align: center; /* Title/Instructor remains centered */
        }


        /* Style for the large-margined text section */
        .project-four-text-section {
            width: 100%;
            box-sizing: border-box;
            /* INCREASED PADDING for 'bigger margins' (200px) */
            padding: 0 200px 30px 200px;
            /* EDITED: LEFT ALIGN THE TEXT */
            text-align: left; 
        }

        /* EDITED: Ensure paragraphs inside the text section are left aligned */
        .project-four-text-section p {
            text-align: left;
        }

        /* Styles for the image gallery rows (3x3x3 layout logic) */
        .image-gallery-row {
            display: flex;
            justify-content: center; /* Center the entire row of images */
            align-items: flex-start;
            gap: 20px; /* Space between images */
            margin-bottom: 30px; /* Space between rows */
            flex-wrap: wrap; /* Important for responsiveness */
        }

        .image-gallery-row .gallery-image-container {
            /* Set a flexible base width for 3 items per row on large screens. */
            flex: 1 1 30%; 
            /* EDITED: Reduced max-width from 350px to 300px (smaller pics) */
            max-width: 300px; 
            min-width: 180px; /* Adjusted min-width for better scaling */
            text-align: center;
        }

        .image-gallery-row .gallery-image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
            cursor: pointer;
        }

        /* --- RESPONSIVENESS FOR MARGINS --- */

        /* Reduce margin/padding for medium screens */
        @media (max-width: 1200px) {
            .project-four-header-content,
            .project-four-text-section {
                padding: 0 50px; 
            }
        }
        /* Minimal padding for mobile screens */
        @media (max-width: 600px) {
            .project-four-header-content,
            .project-four-text-section {
                padding: 0 15px; 
            }
        }

        /* Remove or repurpose unused styles from the original template */
        .project-four-intro-layout { display: none; }
        .project-four-results-row { display: none; }
    </style>

    <div class="project-four-header-content">
        <h1>Dissecting Ion Channel Contributions to Action Potential Dynamics in Lymnaea stagnalis</h1>
        <p style="font-size: 0.9em; margin-top: -5px; margin-bottom: 0px;">
            Note: images taken during recording. Instructor: Dr Jim Deuchars
        </p>
    </div>

    <div class="project-four-text-section">
        <!-- UPDATED TEXT CONTENT START -->
        <p>The Lymnaea Stagnalis is one of the simplest CNS brain models because of its well defined neurons. It has around 20,000 neurons with 11 well-identified interconnected ganglia compared to the 86 billion neurons found in humans. Dissected snail brain (Size = 1-2 mm) and prepared ganglion for recording. Utilized electrophysiological recordings with specific drug perfusion (TEA, Lidocaine, Cadmium). Analyzed recording patterns to see effects of specified drugs on the neurotransmitter release. Skills Gained: microdissection and electrophysiology result analysis.</p>
    </div>

    <!-- Gallery Row 1: 3 Images (New order: 0002, 0003, 0005) -->
    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0002.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0002.jpg" alt="DeepLabCut labeling interface for bee key points." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0003.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0003.jpg" alt="Visualization of the estimated pose key points on the bee." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0005.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0005.jpg" alt="Diagram illustrating the sensory stimulation task apparatus." class="project-image">
            </a>
        </div>
    </div>

    <!-- Gallery Row 2: 3 Images (New order: 0006, 0008, 0009) -->
    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0006.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0006.jpg" alt="Example of a tracked bumblebee path during the task." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0008.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0008.jpg" alt="Final visualization of behavior results." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0009.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0009.jpg" alt="Additional data visualization or experimental result 1." class="project-image">
            </a>
        </div>
    </div>

    <!-- Gallery Row 3: 2 Images (New order: 0004, 0007) - The requested final two -->
    <div class="image-gallery-row">
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0004.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0004.jpg" alt="Graph showing pose estimation accuracy or loss over training." class="project-image">
            </a>
        </div>
        <div class="gallery-image-container">
            <a href="images/IMG-20251016-WA0007.jpg" target="_blank">
                <img src="images/IMG-20251016-WA0007.jpg" alt="Statistical analysis of bee movement metrics." class="project-image">
            </a>
        </div>
    </div>

</section>

</main>
